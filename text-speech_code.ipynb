{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDv7huNYlT2r"
      },
      "source": [
        "## 安装工具包"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSMtfUPLlLrI"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/datasets.git\n",
        "!pip install jiwer\n",
        "!pip install torchaudio\n",
        "#!pip install librosa\n",
        "!pip install  pandas\n",
        "!pip install evaluate\n",
        "#!pip install torch\n",
        "!pip install -U torch-summary\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGQmnokKSWK0",
        "outputId": "9d854d5e-a0d8-4a23-9667-cf3cb3e3e08b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1sYXFRZ5iW3"
      },
      "source": [
        "## 从Google drive 加载数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z92euKeiLTJ",
        "outputId": "3091253b-0e31-44f1-c37e-87f029885e6b"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUmP2rrSehP6"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/iemocap4_longest_nopadding/test.hf /content/sample_data\n",
        "!cp -r /content/drive/MyDrive/iemocap4_longest_nopadding/train.hf /content/sample_data\n",
        "data_path = \"/content/drive/MyDrive/iemocap4_longest_nopadding\"\n",
        "from datasets import load_from_disk\n",
        "tokenized_test_dataset = load_from_disk(\"/content/sample_data/test.hf\")\n",
        "tokenized_train_dataset = load_from_disk(\"/content/sample_data/train.hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsfdFdALmzYa"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/iemocap4_5s/test.hf /content/sample_data\n",
        "!cp -r /content/drive/MyDrive/iemocap4_5s/train.hf /content/sample_data\n",
        "data_path = \"/content/drive/MyDrive/iemocap4_5s\"\n",
        "from datasets import load_from_disk\n",
        "tokenized_test_dataset = load_from_disk(\"/content/sample_data/test.hf\")\n",
        "tokenized_train_dataset = load_from_disk(\"/content/sample_data/train.hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "e1Dd3LMbrd4j",
        "outputId": "eab91ef4-9daa-4df4-b300-bedca9bdf3fb"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "tokenized_test_dataset = load_from_disk(\"/content/sample_data/test.hf\")\n",
        "tokenized_train_dataset = load_from_disk(\"/content/sample_data/train.hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "txiIFWj-oT8X",
        "outputId": "4b85a0c2-bf1a-4cc1-9804-99c9b152e4a1"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset[0][\"input_values\"].type()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJRp7TXGmIQz",
        "outputId": "483f1c2f-8896-470f-8ba8-749a4acfb96f"
      },
      "outputs": [],
      "source": [
        "print(tokenized_train_dataset[0][\"input_values\"].size())\n",
        "print(tokenized_train_dataset[4289][\"input_values\"].size())\n",
        "print(tokenized_test_dataset[160][\"input_values\"].size())\n",
        "print(tokenized_test_dataset[1240][\"input_values\"].size())\n",
        "tokenized_test_dataset.column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG_WWRJ1l4v3"
      },
      "source": [
        "### 处理数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERiT1v1IQx3P"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "!cp -r /content/drive/MyDrive/test.hf /content/sample_data\n",
        "!cp -r /content/drive/MyDrive/train.hf /content/sample_data\n",
        "\n",
        "from datasets import load_from_disk\n",
        "test_dataset = load_from_disk(\"/content/sample_data/test.hf\")\n",
        "train_dataset = load_from_disk(\"/content/sample_data/train.hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "9uWY5z05eZuz",
        "outputId": "5a90058c-0ea6-4e81-b594-a3df7eaef2aa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import librosa\n",
        "from datasets import load_dataset\n",
        "from transformers import HubertForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"wofeishenling/autotrain-iemocap_text_4-39809103601\")\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(lambda example: feature_extractor(example[\"speech\"],\n",
        "                                        sampling_rate=16000,\n",
        "                                        max_length=int(feature_extractor.sampling_rate * 5),\n",
        "                                        truncation='max_length',\n",
        "                                        padding = 'max_length',\n",
        "                                        return_tensors=\"pt\",\n",
        "                                        return_attention_mask = True,\n",
        "                                        ))\n",
        "tokenized_test_dataset = test_dataset.map(lambda example: feature_extractor(example[\"speech\"],\n",
        "                                        sampling_rate=16000,\n",
        "                                        max_length=int(feature_extractor.sampling_rate * 5),\n",
        "                                        truncation='max_length',\n",
        "                                        padding = 'max_length',\n",
        "                                        return_tensors=\"pt\",\n",
        "                                        return_attention_mask = True\n",
        "                                        ))\n",
        "\n",
        "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"attention_mask\", \"attention_mask_audio\")\n",
        "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"attention_mask\", \"attention_mask_audio\")\n",
        "\n",
        "tokenized_train_dataset = tokenized_train_dataset.map(lambda example: tokenizer(example[\"transcription\"], truncation=True, padding='max_length', max_length=128))\n",
        "tokenized_test_dataset = tokenized_test_dataset.map(lambda example: tokenizer(example[\"transcription\"], truncation=True, padding='max_length', max_length=128))\n",
        "\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"file\",\"audio\",\"transcription\",\"speech\"])\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"file\",\"audio\",\"transcription\",\"speech\"])\n",
        "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
        "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7P2XNCw17hL",
        "outputId": "b57b7a8b-6771-4acc-e462-b5f333961980"
      },
      "outputs": [],
      "source": [
        "tokenized_test_dataset.column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6WVygNhpbRf"
      },
      "outputs": [],
      "source": [
        "tokenized_test_dataset.set_format(\"torch\")\n",
        "tokenized_train_dataset.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "p-9Ui4FoDDcF",
        "outputId": "d1be4be9-f15c-432e-9600-bca7aa04a4bb"
      },
      "outputs": [],
      "source": [
        "def reshape_tensor(example):\n",
        "    # Assume 'sample' is a tensor with shape [1, 5]\n",
        "    example[\"input_values\"] = example[\"input_values\"].squeeze(0)\n",
        "    example[\"attention_mask_audio\"] = example[\"attention_mask_audio\"].squeeze(0)\n",
        "    return example\n",
        "tokenized_train_dataset = tokenized_train_dataset.map(reshape_tensor)\n",
        "tokenized_test_dataset = tokenized_test_dataset.map(reshape_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46XE8X2Tl3aZ",
        "outputId": "9ac06cfd-57ae-406d-f7dc-1709a6134e7b"
      },
      "outputs": [],
      "source": [
        "# tokenized_test_dataset.column_names\n",
        "print(tokenized_train_dataset[0][\"input_values\"].size())\n",
        "print(tokenized_train_dataset[1][\"input_values\"].size())\n",
        "print(tokenized_test_dataset[160][\"input_values\"].size())\n",
        "print(tokenized_test_dataset[161][\"input_values\"].size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "nQCW8agJf43d",
        "outputId": "b58fc3f6-6f2a-425d-b2d3-01fc5de05d37"
      },
      "outputs": [],
      "source": [
        "tokenized_test_dataset.save_to_disk(\"/content/drive/MyDrive/iemocap4_5s/test.hf\")\n",
        "tokenized_train_dataset.save_to_disk(\"/content/drive/MyDrive/iemocap4_5s/train.hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAB7s2HoRBxa",
        "outputId": "7cb9d8c3-4ea5-4cc8-f918-f1afec907162"
      },
      "outputs": [],
      "source": [
        "len_list = []\n",
        "cnt = 0\n",
        "for e in tokenized_train_dataset['input_values']:\n",
        "  t = int(e.size()[0])/16000\n",
        "  if t>15:\n",
        "    cnt=cnt+1\n",
        "  len_list.append(t)\n",
        "print(cnt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqaAkTdtmy65"
      },
      "source": [
        "## TEXT-ONLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xxcv5WqnLnk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, AutoModelForAudioClassification, AutoModelForPreTraining, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import librosa\n",
        "from transformers import AutoModel\n",
        "from transformers import BertModel\n",
        "import math\n",
        "import os\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Union\n",
        "import torch.nn.init as init\n",
        "import torch.utils.checkpoint\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "class text_only_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(text_only_model, self).__init__()\n",
        "        # 选择并加载baseModel\n",
        "        self.bert_base = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states = True)\n",
        "        #self.bert_base = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/myModel/bert_12layers\", output_hidden_states=True)\n",
        "        #self.bert_base = AutoModelForSequenceClassification.from_pretrained(\"JerryM/distilbert-base-uncased-finetuned-emotion\",output_hidden_states=True)\n",
        "        #self.bert_base = AutoModelForSequenceClassification.from_pretrained(\"wofeishenling/autotrain-iemocap_text_4-39809103601\", output_hidden_states = True)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(768, 4)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        #token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        #print(model.M_12_1.weight)\n",
        "        outputs = self.bert_base(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            #token_type_ids=token_type_ids,\n",
        "            #position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        hidden_states = outputs.hidden_states\n",
        "        #print(hidden_states[1][:, 0, :].shape) #32*768\n",
        "        # 取出模型的1-12层输出\n",
        "\n",
        "        #取第12层作为分类器的输入\n",
        "        logits = self.classifier(hidden_states[12][:, 0, :])\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        loss = loss_fct(logits.view(-1, 4), labels.view(-1))\n",
        "\n",
        "        output = (logits,)\n",
        "        return ((loss,) + output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLvJ9ryGnkSr"
      },
      "outputs": [],
      "source": [
        "# 实例化模型\n",
        "model = text_only_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xU2LrtZEcII"
      },
      "outputs": [],
      "source": [
        "# 冻结bert模型的参数，使其不参与参数的更新\n",
        "for param in model.bert_base.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9cnq-qcg0Y_"
      },
      "outputs": [],
      "source": [
        "import torchsummary\n",
        "torchsummary.summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnC6CuV_xA1t"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1xmTd07d-aU"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# 设置训练参数\n",
        "class CustomTrainer(Trainer):\n",
        "    def create_optimizer(self):\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "              \"params\": self.model.classifier.parameters(),\n",
        "              \"lr\": 1e-3,\n",
        "              \"weight_decay\": self.args.weight_decay\n",
        "            },\n",
        "            {\n",
        "              \"params\": [self.model.w1, self.model.w2],\n",
        "              \"lr\": 0.005,\n",
        "              \"weight_decay\": self.args.weight_decay\n",
        "            }\n",
        "        ]\n",
        "        self.optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate)\n",
        "        return self.optimizer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\", evaluation_strategy=\"epoch\",\n",
        "    learning_rate = 5e-5,\n",
        "    weight_decay = 0.1,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50MXQW3Hn6gy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "# 设置评价指标\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg5VcfJ_oBrM"
      },
      "outputs": [],
      "source": [
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "2urdazQAoCho",
        "outputId": "2aff76e5-8cd3-4d31-836a-2ae63d025b7f"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcDxXCH9xGTN"
      },
      "source": [
        "### 自定义trian\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lHhBls6xfG1"
      },
      "outputs": [],
      "source": [
        "num_epochs = 3\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_train_dataset, shuffle=True, batch_size=8)\n",
        "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=8)\n",
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "from transformers import get_scheduler\n",
        "\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6QdwzUX07LT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaKu3yFS0pQq"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        outputs = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), torch.tensor(batch['labels'].to(device)))\n",
        "        loss = outputs[0]\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vArsxaXe0uAD",
        "outputId": "ebc07c31-7005-43b1-ab8c-52f6e722df97"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "model.eval()\n",
        "for batch in test_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs[1]\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPwa3yaA46N-"
      },
      "source": [
        "## AUDIO-ONLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMosT2EB6V5M"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, AutoModelForAudioClassification, AutoModelForPreTraining, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import librosa\n",
        "from transformers import HubertForSequenceClassification, Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n",
        "import torch.nn.init as init\n",
        "from transformers import AutoModel\n",
        "import math\n",
        "import os\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "class audio_only_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(audio_only_model, self).__init__()\n",
        "        # 选择basemodel并加载\n",
        "        #--------------local_model---------------\n",
        "        #self.model = HubertForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/myModel/hubert\", output_hidden_states = True)\n",
        "        #self.model = Wav2Vec2ForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/myModel/wav2vec2_superb\", output_hidden_states = True)\n",
        "        #self.model = HubertForSequenceClassification.from_pretrained(\"superb/hubert-base-superb-er\", output_hidden_states = True)\n",
        "        #self.model = AutoModelForPreTraining.from_pretrained(\"facebook/wav2vec2-base-960h\", output_hidden_states = True)\n",
        "        self.model = AutoModelForPreTraining.from_pretrained(\"facebook/wav2vec2-base\", output_hidden_states = True)\n",
        "        #self.model = Wav2Vec2ForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/myModel/wav2vec2_superb_12layers\", output_hidden_states = True)\n",
        "        #self.model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-er\", output_hidden_states = True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.classifier = nn.Linear(768, 4)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_values: Optional[torch.Tensor] = None,\n",
        "        attention_mask_audio: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        #print(input_values.size())\n",
        "        outputs = self.model(input_values, attention_mask=attention_mask_audio)\n",
        "        hidden_states = outputs.hidden_states\n",
        "        # 取出模型的1-12层输出\n",
        "        hidden_M = [\n",
        "            #hidden_states[0][:, 0, :],\n",
        "            hidden_states[1][:, 0, :],\n",
        "            hidden_states[2][:, 0, :],\n",
        "            hidden_states[3][:, 0, :],\n",
        "            hidden_states[4][:, 0, :],\n",
        "            hidden_states[5][:, 0, :],\n",
        "            hidden_states[6][:, 0, :],\n",
        "            hidden_states[7][:, 0, :],\n",
        "            hidden_states[8][:, 0, :],\n",
        "            hidden_states[9][:, 0, :],\n",
        "            hidden_states[10][:, 0, :],\n",
        "            hidden_states[11][:, 0, :],\n",
        "            hidden_states[12][:, 0, :],\n",
        "        ]\n",
        "        # 取第1层作为分类器的输入\n",
        "        logits = self.classifier(hidden_states[1][:, 0, :])\n",
        "\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        loss = loss_fct(logits.view(-1, 4), labels.view(-1))\n",
        "\n",
        "        output = (logits,)\n",
        "        return ((loss,) + output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UApdhCZuI2LC",
        "outputId": "f24763e5-9117-4df5-c9f2-5508a4c75aed"
      },
      "outputs": [],
      "source": [
        "m = audio_only_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2NLkVT-t9EZ"
      },
      "outputs": [],
      "source": [
        "# 冻结参数\n",
        "for param in m.model.parameters():\n",
        "   param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "Q-7FJ2pPQNvz",
        "outputId": "ea9707d5-9c61-4fd7-8c8c-ed9982a31e76"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wGFd1iWibGM"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate = 1e-3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    fp16 = True,\n",
        "    output_dir=\"wofeishenling/wofei\",\n",
        "    #push_to_hub=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPOtPZd4Qscc"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=m,\n",
        "    #model = AutoModelForPreTraining.from_pretrained(\"facebook/wav2vec2-base-960h\",\n",
        "    #                        output_hidden_states = True,\n",
        "    #                        num_labels = 4),\n",
        "    args=training_args,\n",
        "    # data_collator = data_collator,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3), SaveBestModelCallback()]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "_9gAVkCEQvtn",
        "outputId": "627645af-dbed-4cc7-bdc8-239e9764e274"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkpzCy8KgG9S"
      },
      "outputs": [],
      "source": [
        "m.model.save_pretrained('/content/drive/MyDrive/myModel/wav2vec2_superb_12layers')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OSau8QymEjA"
      },
      "source": [
        "## FUSION MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sthl-WDVmEDr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import librosa\n",
        "from datasets import load_dataset\n",
        "from transformers import HubertForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import HubertForSequenceClassification, Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUAb85mSf_NQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# 简单融合\n",
        "class Concatenation(nn.Module):\n",
        "    def __init__(self, feature_size=768):\n",
        "        super(Concatenation, self).__init__()\n",
        "        self.classifier = nn.Linear(768*2, 768)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        cat_features = torch.cat((audio_features, text_features), dim=1)\n",
        "        cat_features = self.classifier(cat_features)\n",
        "\n",
        "        return cat_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRnWqltAgDGF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "d_model = 768\n",
        "nhead = 8\n",
        "dropout = 0.1\n",
        "layer_norm_eps = 1e-5\n",
        "dim_feedforward = 3072\n",
        "# 使用注意力机制融合模块\n",
        "class CoAttention(nn.Module):\n",
        "    def __init__(self, feature_size=768):\n",
        "        super(CoAttention, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "\n",
        "    def forward(self, af, tf):\n",
        "        x = self.norm1(af + self._sa_block(af, tf, tf))\n",
        "        x = self.norm2(x + self._ff_block(x))\n",
        "\n",
        "        y = self.norm1(tf + self._sa_block(tf, af, af))\n",
        "        y = self.norm2(y + self._ff_block(y))\n",
        "\n",
        "        x1 = self.norm1(x + self._sa_block(x, y, y))\n",
        "        x1 = self.norm2(x1 + self._ff_block(x1))\n",
        "\n",
        "        y1 = self.norm1(y + self._sa_block(y, x, x))\n",
        "        y1 = self.norm2(y1 + self._ff_block(y1))\n",
        "\n",
        "        x2 = self.norm1(x1 + self._sa_block(x1, y1, y1))\n",
        "        x2 = self.norm2(x2 + self._ff_block(x2))\n",
        "\n",
        "        y2 = self.norm1(y1 + self._sa_block(y1, x1, x1))\n",
        "        y2 = self.norm2(y2 + self._ff_block(y2))\n",
        "\n",
        "        fused_features = (x+y)/2\n",
        "        return fused_features\n",
        "\n",
        "    def _sa_block(self, q, k, v):\n",
        "        x = self.self_attn(q, k, v, need_weights=False)[0]\n",
        "        return self.dropout1(x)\n",
        "\n",
        "    # feed forward block\n",
        "    def _ff_block(self, x):\n",
        "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
        "        return self.dropout2(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u-yn9W0SYAQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, AutoModelForAudioClassification, AutoModelForPreTraining, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "from transformers import BertModel\n",
        "import math\n",
        "import os\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FusionModel, self).__init__()\n",
        "\n",
        "        # 分别选择文本basemodel以及语音的basemodel\n",
        "        #self.text_model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states = True)\n",
        "        #self.audio_model = AutoModelForPreTraining.from_pretrained(\"facebook/wav2vec2-base\", output_hidden_states = True)\n",
        "        self.audio_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/myModel/wav2vec2_superb\", output_hidden_states = True)\n",
        "        #self.audio_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base\", output_hidden_states = True)\n",
        "        self.text_model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/myModel/bert_12layers\", output_hidden_states=True)\n",
        "\n",
        "        # 选择融合机制\n",
        "        self.fusion_model = CoAttention()\n",
        "        #self.fusion_model = Concatenation()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear1 = nn.Linear(768*2, 4)\n",
        "        self.linear = nn.Linear(768, 4) # output features from bert is 768 and 2 is ur number of labels\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_values: Optional[torch.Tensor] = None,\n",
        "        attention_mask_audio: Optional[torch.Tensor] = None,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        outputs_audio = self.audio_model(input_values, attention_mask=attention_mask_audio)\n",
        "        outputs_text  = self.text_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        ## 指定融合的匹配策略\n",
        "        #layer_fusion1 = self.fusion_model(outputs_audio.hidden_states[1][:,0,:],outputs_text.hidden_states[1][:,0,:])\n",
        "        #layer_fusion2 = self.fusion_model(outputs_audio.hidden_states[1][:,0,:],outputs_text.hidden_states[5][:,0,:])\n",
        "        #layer_fusion3 = self.fusion_model(outputs_audio.hidden_states[2][:,0,:],outputs_text.hidden_states[6][:,0,:])\n",
        "        #layer_fusion4 = self.fusion_model(outputs_audio.hidden_states[3][:,0,:],outputs_text.hidden_states[7][:,0,:])\n",
        "        layer_fusion5 = self.fusion_model(outputs_audio.hidden_states[8][:,0,:],outputs_text.hidden_states[1][:,0,:])\n",
        "        layer_fusion6 = self.fusion_model(outputs_audio.hidden_states[9][:,0,:],outputs_text.hidden_states[2][:,0,:])\n",
        "        layer_fusion7 = self.fusion_model(outputs_audio.hidden_states[10][:,0,:],outputs_text.hidden_states[3][:,0,:])\n",
        "        layer_fusion8 = self.fusion_model(outputs_audio.hidden_states[11][:,0,:],outputs_text.hidden_states[4][:,0,:])\n",
        "        layer_fusion9 = self.fusion_model(outputs_audio.hidden_states[12][:,0,:],outputs_text.hidden_states[5][:,0,:])\n",
        "        #layer_fusion10 = self.fusion_model(outputs_audio.hidden_states[1][:,0,:],outputs_text.hidden_states[10][:,0,:])\n",
        "        #layer_fusion11 = self.fusion_model(outputs_audio.hidden_states[7][:,0,:],outputs_text.hidden_states[11][:,0,:])\n",
        "        #layer_fusion12 = self.fusion_model(outputs_audio.hidden_states[8][:,0,:],outputs_text.hidden_states[12][:,0,:])\n",
        "\n",
        "        #outputs_fusion = self.w1*layer_fusion1 + self.w2*layer_fusion2 + self.w3*layer_fusion3 + self.w4*layer_fusion4 + self.w5*layer_fusion5 + self.w6*layer_fusion6 + self.w7*layer_fusion7 + self.w8*layer_fusion8 + self.w9*layer_fusion9 + self.w10*layer_fusion10 + self.w11*layer_fusion11 + self.w12*layer_fusion12\n",
        "        outputs_fusion = (layer_fusion5 + layer_fusion6 + layer_fusion7 + layer_fusion8 + layer_fusion9)/5.0\n",
        "        # + layer_fusion10 + layer_fusion11 + layer_fusion12\n",
        "        #outputs_fusion = 0.0002443*layer_fusion1 + 0.0004886*layer_fusion2 + 0.0009772*layer_fusion3 + 0.0019544*layer_fusion4 + 0.0039088*layer_fusion5 + 0.0078176*layer_fusion6 + 0.0156352*layer_fusion7 + 0.0312704*layer_fusion8 + 0.0625408*layer_fusion9 + 0.1250816*layer_fusion10 + 0.2501632*layer_fusion11 + 0.5003264*layer_fusion12\n",
        "\n",
        "\n",
        "        # 将融合好的特征进行分类\n",
        "        logits = self.linear(outputs_fusion)\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        loss = loss_fct(logits.view(-1, 4), labels.view(-1))\n",
        "\n",
        "        output = (logits,)\n",
        "        return ((loss,) + output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uGCihlG4gHi",
        "outputId": "1069e1f2-b236-4a87-841a-f53fdc3c4f82"
      },
      "outputs": [],
      "source": [
        "model = FusionModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRKYaTgIr1fc"
      },
      "outputs": [],
      "source": [
        "for param in model.audio_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.text_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g0gb-Cup01G"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\", evaluation_strategy=\"epoch\",\n",
        "    learning_rate = 5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAm2U9M_p5l2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from sklearn.metrics import accuracy_score\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_opiZpoU7kNu"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def create_optimizer(self):\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "              \"params\": self.model.linear.parameters(),\n",
        "              \"lr\": 1e-3,\n",
        "              \"weight_decay\": self.args.weight_decay\n",
        "            },\n",
        "            {\n",
        "              \"params\": self.model.fusion_model.parameters(),\n",
        "              \"lr\": self.args.learning_rate,\n",
        "              \"weight_decay\": self.args.weight_decay\n",
        "            }\n",
        "        ]\n",
        "        self.optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate)\n",
        "        return self.optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjeW-uDZirlL"
      },
      "outputs": [],
      "source": [
        "class MyCallback(TrainerCallback):\n",
        "    \"A callback that prints a message at the beginning of training\"\n",
        "    # epoch = 1\n",
        "    def on_epoch_begin(self, args, state, control, model, **kwargs):\n",
        "      model.audio_model.save_pretrained(f'/content/drive/MyDrive/myModel/skew4/wav2vec2_{state.epoch}_epoch')\n",
        "      model.text_model.save_pretrained(f'/content/drive/MyDrive/myModel/skew4/bert_{state.epoch}_epoch')\n",
        "      #self.epoch = self.epoch + 1\n",
        "      print(f\"save{state.epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aa4V7kdp_qC",
        "outputId": "bbdda917-52e3-4799-cab3-f5e5494aa8f2"
      },
      "outputs": [],
      "source": [
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    # data_collator=data_collator,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    #tokenizer = feature_extractor,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "5-6ItmY3qBUm",
        "outputId": "b9e0d500-e22a-4695-b30d-a8a0962e3859"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JbUYjeRU1a2"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jDv7huNYlT2r",
        "fG_WWRJ1l4v3",
        "OqaAkTdtmy65",
        "qcDxXCH9xGTN",
        "gPwa3yaA46N-",
        "JJKXq3HnnLGV",
        "8HUkUtvolxOA"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
